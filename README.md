# text-mining

Please read the [instructions](instructions.md).


I used Wikipedia as my data source.  I let the users enter the Wikipedia pages they wanted to compare and created an error statement that prompted the user to try again if the page they entered did not exist.  I used word frequencies and text similarities to compare the pages.  I wanted to learn more about how to compare different pages as well as become familiar with having the user search through the page in preperation for my term project.

I needed to figure out how to get rid of all of the punctuation and meaningless words when looking at word frequency.  I also wanted to make the code quick and friendly for the user.  At first, I just showed all of the words with the number of times each one appeared, but this was messy.  I then considered showing the top ten most frequent words that appeared on the Wikipedia page their frequency.  However, most of the words that we frequent were "the" and "of" which offer the user no benefit.  Hence, I decided to just show the number of words on the Wikipedia page and the number of times the title word appears.  
I also noticed that when I would miss-spell a word when inputing a Wikipedia title, I would get an error and a huge chunk of text.  I realized that this would be confusing for the user.  Hence, I decided to create an error message loop which would then allow the user to re-enter a Wikipedia page.

In the end, this program shows the number of words on each Wikipedia page, the number of times the title shows on the Wikipedia page, and the Fuzz Ratio based on the two Wikipedia pages that the user enters.  After playing around with this program I have not been able to find a Fuzz Ratio below 4.  One interesting example I noticed was that "politics" and "grape" have a Fuzz Ratio of 34 which is higher than I expected.  
I also realized how much larger some Wikipedia pages are than others.  For instance, the Wikipedia page for "water" has 63,945 words while the Wikipedia page for "basket" only has 3,481 words.  Being able to compare Wikipedia pages this easily is fascinating because it would typically require a user to copy and paste the text into a word counter application.  This help the user get a sense of how much content is on a webpage and how the words compare.

Being able to compare Wikipedia pages easily struck me.  I had not been able to grasp how useful python can be until this assignment.  While this was a challenging assignment, I felt that it was not unreasonable.  Looking back on past assignments was useful and focusing on working through problems by finding and trying different ways to solve them was fun.  I think that the project was appropriately scoped.  If I had more time I would have liked to allow users to enter as many Wikipedia pages as they wanted and created a table to compare the results.  Throughout my entire writing of the code, I constantly printed and always started with simple code which worked well.  Going forward it was a good idea to write out my thoughts in docstring to help me understand where I should start.  I made myself not write any code for the first 1.5 hours, only docstring.  Before starting this project, I wish I had gotten familiar with ways to analyze the text outside of what you offered.  It would have been interesting to look at this because some of the strategies to analyze the text that you offered were not relevant to my project.